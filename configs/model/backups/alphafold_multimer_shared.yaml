globals:
  num_recycle: 20

data:
  common:
    masked_msa:
      profile_prob: 0.1
      same_prob: 0.1
      uniform_prob: 0.1
    max_extra_msa: 1152
    msa_cluster_features: true
    num_recycle: ${global.num_recycle}
    reduce_msa_clusters_by_max_templates: false
    resample_msa_in_recycling: true
    use_templates: false
  predict:
    fixed_size: true # Pad the inputs to a uniform size.
    subsample_templates: false
    block_delete_msa: false
    random_delete_msa: true
    masked_msa_replace_fraction: 0.15
    max_msa_clusters: 128
    max_template_hits: 4
    max_templates: 4
    num_ensemble: 2
    crop: false
    crop_size: null
    supervised: false
    biased_msa_by_chain: false
  eval:
    fixed_size: true
    subsample_templates: false
    block_delete_msa: false
    random_delete_msa: true
    masked_msa_replace_fraction: 0.15
    max_msa_clusters: 128
    max_template_hits: 4
    max_template: 4
    num_ensemble: 1
    crop: false
    crop_size: null
    supervised: true
    biased_msa_by_chain: false
  train:
    fixed_size: true
    subsample_templates: true
    block_delete_msa: true
    random_delete_msa: true
    masked_msa_replace_fraction: 0.15
    max_msa_clusters: 128
    max_template_hits: 4
    max_template: 4
    num_ensemble: 1
    crop: true
    crop_size: 384
    spatial_crop_prob: 0.5
    supervised: true
    use_clamped_fape_prob: 1.0
    max_distillation_msa_cluster: 1000
    biased_msa_by_chain: true
    share_mask: true

model:
  # If true, swap the order of the attention and triangular multiplicative update layers in the
  # template stack. Also, move the outer product mean to the start of the Evoformer block.
  is_multimer: true
  embeddings_and_evoformer:
    evoformer_num_block: 48
    evoformer:
      # If true, override module-wise dropout rate.
      shared_dropout: true
      msa_dropout_rate: 0.15
      pair_dropout_rate: 0.25
      msa_row_attention_with_pair_bias:
        dropout_rate: 0.15
        gating: true
        num_head: 8
        orientation: per_row
        shared_dropout: true
      msa_column_attention:
        dropout_rate: 0.0
        gating: true
        num_head: 8
        orientation: per_column
        shared_dropout: true
      msa_transition:
        dropout_rate: 0.0
        num_intermediate_factor: 4
        orientation: per_row
        shared_dropout: true
      outer_product_mean:
        chunk_size: 128
        dropout_rate: 0.0
        num_outer_channel: 32
        orientation: per_row
        shared_dropout: true
      triangle_attention_starting_node:
        dropout_rate: 0.25
        gating: true
        num_head: 4
        orientation: per_row
        shared_dropout: true
      triangle_attention_ending_node:
        dropout_rate: 0.25
        gating: true
        num_head: 4
        orientation: per_column
        shared_dropout: true
      triangle_multiplication_outgoing:
        dropout_rate: 0.25
        equation: ikc,jkc->ijc
        num_intermediate_channel: 128
        orientation: per_row
        shared_dropout: true
        fuse_projection_weights: true
      triangle_multiplication_incoming:
        dropout_rate: 0.25
        equation: kjc,kic->ijc
        orientation: per_row
        shared_dropout: true
        fuse_projection_weights: true
      pair_transition:
        dropout_rate: 0.0
        num_intermediate_factor: 4
        orientation: per_row
        shared_dropout: true
    extra_msa:
      enabled: true
      shared_dropout: true
      msa_dropout_rate: 0.15
      pair_dropout_rate: 0.25
      extra_msa_stack:
        num_channel: 64 # c_e
        num_block: 4
        fuse_projection_weights: true
    use_chain_relative: true
    max_relative_chain: 2
    max_relative_idx: 32
    msa_channel: 256
    pair_channel: 128
    prev_pos:
      min_bin: 3.25
      max_bin: 20.75
      num_bins: 15
    recycle_features: true
    recycle_pos: true
    seq_channel: 384
    template:
      attention:
        gating: false
        key_dim: 64
        num_head: 4
        value_dim: 64
      dgram_features:
        min_bin: 3.25
        max_bin: 50.75
        num_bins: 39
      embed_torsion_angles: false
      enabled: true
      template_pair_stack:
        num_block: 2
        tri_attn_first: true
        triangle_attention_starting_node:
          dropout_rate: 0.25
          gating: true
          key_dim: 64
          num_head: 4
          orientation: per_row
          shared_dropout: true
          value_dim: 64
        triangle_attention_ending_node:
          dropout_rate: 0.25
          gating: true
          key_dim: 64
          num_head: 4
          orientation: per_column
          shared_dropout: true
          value_dim: 64
        triangle_multiplication_outgoing:
          dropout_rate: 0.25
          equation: ikc,jkc->ijc
          num_intermediate_channel: 64
          orientation: true
          fuse_projection_weights: true
        triangle_multiplication_incoming:
          dropout_rate: 0.25
          equation: kjc,kic->ijc
          num_intermediate_channel: 64
          orientation: per_row
          shared_dropout: true
          fuse_projection_weights: true
        pair_transition:
          dropout_rate: 0.0
          num_intermediate_factor: 2
          orientation: per_row
          shared_dropout: true
      max_templates: 4
      subbatch_size: 128
      use_templates_unit_vector: false

  global_config:
    bfloat16: true
    bfloat_output: false
    deterministic: false
    multimer_mode: true
    subbatch_size: 4
    use_remat: false
    zero_init: true
    eval_dropout: false

  heads:
    distogram:
      first_break: 2.3125
      last_break: 21.6875
      num_bins: 64
      weight: 0.3
    predicted_aligned_error:
      max_error_bin: 31.0
      num_bins: 64
      num_channels: 128
      filter_by_resolution: true
      min_resolution: 0.1
      max_resolution: 3.0
      weight: 0.1
    experimentally_resolved:
      enabled: false # true when fine-tuning
      filter_by_resolution: true
      min_resolution: 0.1
      max_resolution: 3.0
      weight: 0.01
    structure_module:
      num_layer: 8
      fape:
        clamp_distance: 10.0
        clamp_type: relu
        loss_unit_distance: 10.0
        clamp_distance_between_chains: 30.0
        loss_unit_distance_between_chains: 20.0
      angle_norm_weight: 0.01
      average_clash_loss_over_clashing_atoms: true # Average over clashing atoms
      bond_angle_loss_weight: 0.3 # From 1.0
      chain_center_of_mass_weight: 1.0 # Additional
      chi_weight: 0.5
      clash_overlap_tolerance: 1.5
      compute_in_graph_metrics: true
      dropout: 0.1
      num_channel: 384
      num_head: 12
      num_layer_in_transition: 3
      num_point_qk: 4
      num_point_v: 8
      num_scalar_qk: 16
      num_scalar_v: 16
      position_scale: 10.0
      sidechain:
        atom_clamp_distance: 10.0
        num_channel: 128
        num_residual_block: 2
        weight_frac: 0.5
        length_scale: 10.0
      structural_violation_loss_weight: 0.03 # From 1.0
      violation_tolerance_factor: 12.0
      weight: 1.0
    preidcted_lddt:
      filter_by_resolution: true
      min_resolution: 0.1
      max_resolution: 3.0
      num_bins: 50
      num_channels: 128
      weight: 0.01
    masked_msa:
      num_output: 23
      weight: 2.0

  num_recycle: ${global.num_recycle}
  resample_msa_in_recycling: true

loss: null
